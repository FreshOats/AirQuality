{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c73514f-5c4b-4c0f-9afb-1d0d8a831642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingestion of Wildfire Data\n",
    "1. This ingests data for 3 days, which includes the current day and two days prior. This will ensure that all delayed data is collected, and may also give insights to the region prior to detection\n",
    "2. The data is partitioned by acquisition date and then zordered by longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "794b6b6e-3192-423c-923e-7ccd930da068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed6f903b-945d-42b8-9c61-8012e23da4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from delta.tables import DeltaTable\n",
    "# import pandas as pd\n",
    "\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# api_key = os.getenv('FIRMS_API_KEY')\n",
    "\n",
    "# # API details\n",
    "# api_key = api_key\n",
    "# dataset = \"VIIRS_NOAA20_NRT\"  # VIIRS NOAA-20 Near Real-Time data\n",
    "# country_code = \"USA\"          # Country code for the United States\n",
    "# days = 2                      # Number of days of data\n",
    "\n",
    "# # Construct the API URL\n",
    "# api_url = f'https://firms.modaps.eosdis.nasa.gov/api/country/csv/{api_key}/{dataset}/{country_code}/{days}'\n",
    "\n",
    "# # Fetch the new data from the FIRMS API\n",
    "# new_data_df = pd.read_csv(api_url)\n",
    "\n",
    "# # Convert the Pandas DataFrame to a Spark DataFrame\n",
    "# new_spark_df = spark.createDataFrame(new_data_df)\n",
    "\n",
    "# # Set the table name to be used in the metastore\n",
    "# table_name = \"firms_data\"\n",
    "\n",
    "# # Check if the table already exists using the Spark catalog\n",
    "# if spark.catalog.tableExists(table_name):\n",
    "#     # Load the existing Delta table using its table name\n",
    "#     delta_table = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "#     # Merge new data with the existing table\n",
    "#     delta_table.alias(\"target\").merge(\n",
    "#         new_spark_df.alias(\"source\"),\n",
    "#         \"\"\"\n",
    "#         target.latitude = source.latitude AND\n",
    "#         target.longitude = source.longitude AND\n",
    "#         target.acq_date = source.acq_date AND\n",
    "#         target.acq_time = source.acq_time\n",
    "#         \"\"\"\n",
    "#     ).whenMatchedUpdateAll() \\\n",
    "#      .whenNotMatchedInsertAll() \\\n",
    "#      .execute()\n",
    "\n",
    "#     # Optimize the table (if your environment supports it)\n",
    "#     spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (longitude)\")\n",
    "# else:\n",
    "#     # If the table doesn't exist, create it as a managed Delta table with partitioning\n",
    "#     new_spark_df.write.format(\"delta\") \\\n",
    "#         .partitionBy(\"acq_date\") \\\n",
    "#         .mode(\"overwrite\") \\\n",
    "#         .saveAsTable(table_name)\n",
    "\n",
    "#     # Optimize after initial write\n",
    "#     spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (longitude)\")\n",
    "\n",
    "# print(\"Data merged and Delta table optimized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5043d675-bfa9-4ae9-b8dd-3091967dfa96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import time\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "api_key = os.getenv('FIRMS_API_KEY')\n",
    "\n",
    "# API details\n",
    "api_key = api_key\n",
    "dataset = \"VIIRS_NOAA20_NRT\"  # VIIRS NOAA-20 Near Real-Time data\n",
    "country_code = \"USA\"          # Country code for the United States\n",
    "days = 3                      # Number of days of data\n",
    "table_name = \"firms_data\"     # Delta table name\n",
    "\n",
    "# Function to fetch data with retry logic\n",
    "def fetch_data_with_retries(api_url, retries=3, backoff_factor=1, timeout=60):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],  # Retry on these HTTP status codes\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "\n",
    "    try:\n",
    "        response = session.get(api_url, timeout=timeout)  # Set a timeout for the request\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        return response.content\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to split the days into smaller chunks\n",
    "def split_days(total_days, chunk_size):\n",
    "    return [range(i, min(i + chunk_size, total_days + 1)) for i in range(1, total_days + 1, chunk_size)]\n",
    "\n",
    "# Main function to orchestrate data collection and merging\n",
    "def collect_and_merge_data():\n",
    "    # Split the days into smaller chunks (e.g., 1 day per chunk)\n",
    "    day_chunks = split_days(days, chunk_size=1)\n",
    "\n",
    "    all_data = []  # To store all the collected data\n",
    "\n",
    "    for chunk in day_chunks:\n",
    "        print(f\"Processing days: {list(chunk)}\")\n",
    "        for day in chunk:\n",
    "            # Construct the API URL for the specific day\n",
    "            api_url = f'https://firms.modaps.eosdis.nasa.gov/api/country/csv/{api_key}/{dataset}/{country_code}/{day}'\n",
    "            try:\n",
    "                # Fetch data with retry logic\n",
    "                csv_data = fetch_data_with_retries(api_url, retries=3, backoff_factor=2, timeout=60)\n",
    "                if csv_data:\n",
    "                    # Read the CSV data into a Pandas DataFrame\n",
    "                    daily_data = pd.read_csv(pd.compat.StringIO(csv_data.decode('utf-8')))\n",
    "                    all_data.append(daily_data)\n",
    "                    print(f\"Successfully fetched data for day {day}\")\n",
    "                else:\n",
    "                    print(f\"Failed to fetch data for day {day}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error for day {day}: {e}\")\n",
    "                continue  # Skip to the next day if there's an error\n",
    "\n",
    "        # Add a delay between chunks to avoid overwhelming the API\n",
    "        time.sleep(5)\n",
    "\n",
    "    # Combine all the collected data into a single DataFrame\n",
    "    if all_data:\n",
    "        new_data_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "        # Convert the Pandas DataFrame to a Spark DataFrame\n",
    "        new_spark_df = spark.createDataFrame(new_data_df)\n",
    "\n",
    "        # Check if the Delta table already exists\n",
    "        if spark.catalog.tableExists(table_name):\n",
    "            # Load the existing Delta table\n",
    "            delta_table = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "            # Merge new data with the existing table\n",
    "            delta_table.alias(\"target\").merge(\n",
    "                new_spark_df.alias(\"source\"),\n",
    "                \"\"\"\n",
    "                target.latitude = source.latitude AND\n",
    "                target.longitude = source.longitude AND\n",
    "                target.acq_date = source.acq_date AND\n",
    "                target.acq_time = source.acq_time\n",
    "                \"\"\"\n",
    "            ).whenMatchedUpdateAll() \\\n",
    "             .whenNotMatchedInsertAll() \\\n",
    "             .execute()\n",
    "\n",
    "            # Optimize the table (if your environment supports it)\n",
    "            spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (longitude)\")\n",
    "        else:\n",
    "            # If the table doesn't exist, create it as a managed Delta table with partitioning\n",
    "            new_spark_df.write.format(\"delta\") \\\n",
    "                .partitionBy(\"acq_date\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .saveAsTable(table_name)\n",
    "\n",
    "            # Optimize after initial write\n",
    "            spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (longitude)\")\n",
    "\n",
    "        print(\"Data merged and Delta table optimized.\")\n",
    "    else:\n",
    "        print(\"No data was collected.\")\n",
    "\n",
    "# Run the function\n",
    "collect_and_merge_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af181fc7-5f41-4c50-9396-4fe4e35a7fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Validate that there are no duplicates or missing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e4c4732-4ba0-4e83-a760-855ce2e0acae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SELECT * \n",
    "# FROM firms_data\n",
    "# WHERE acq_date = '2025-03-05' AND confidence = 'h'\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze 1 - FIRMS Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
