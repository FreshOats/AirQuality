{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ca5cb4d-779e-4e56-a5e2-5d6b34076132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This calls the OpenAQ API and collects the latest data from location IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc905c85-d64d-4ebb-a425-be051b0c4e1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c198f786-1cbc-46a4-8cdd-cad8cc64e419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# import requests\n",
    "# import json\n",
    "# import random\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# api_key = os.getenv('OPENAQ_API_KEY')\n",
    "\n",
    "# # Set your API details\n",
    "# API_KEY = api_key\n",
    "# BASE_URL = \"https://api.openaq.org/v3\"\n",
    "\n",
    "\n",
    "# # Load the static Parquet file\n",
    "# distinct_sensors_path = \"/Volumes/tabular/dataexpert/freshoats_capstone/la_sensors_delta_table\"\n",
    "# distinct_sensors_df = spark.read.format('delta').load(distinct_sensors_path)\n",
    "\n",
    "# # Retrieve the list of location IDs from your Spark DataFrame\n",
    "# location_ids = distinct_sensors_df.select(\"location_id\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# # List to accumulate the latest sensor data for each location\n",
    "# latest_sensor_data = []\n",
    "\n",
    "# max_attempts = 5\n",
    "\n",
    "# for location_id in location_ids:\n",
    "#     url = f\"{BASE_URL}/locations/{location_id}/latest\"\n",
    "#     headers = {\"X-API-Key\": API_KEY}\n",
    "    \n",
    "#     attempt = 0\n",
    "#     retry_time = 0.5\n",
    "#     success = False\n",
    "    \n",
    "#     while attempt < max_attempts:\n",
    "#         try:\n",
    "#             response = requests.get(url, headers=headers)\n",
    "#             response.raise_for_status()\n",
    "#             data = response.json()\n",
    "\n",
    "#             # Convert the nested \"results\" field to a JSON string to avoid schema inference issues\n",
    "#             if \"results\" in data:\n",
    "#                 data[\"results\"] = json.dumps(data[\"results\"])\n",
    "\n",
    "#             # Optionally add location_id if it isn't already in the response\n",
    "#             if \"location_id\" not in data:\n",
    "#                 data[\"location_id\"] = location_id\n",
    "\n",
    "#             latest_sensor_data.append(data)\n",
    "#             print(f\"Successfully fetched data for location {location_id}\")\n",
    "#             success = True\n",
    "#             break\n",
    "\n",
    "#         except requests.exceptions.HTTPError as http_err:\n",
    "#             if response.status_code == 429:\n",
    "#                 # Rate limiting: pause using the Retry-After header if available\n",
    "#                 retry_after = response.headers.get(\"Retry-After\")\n",
    "#                 sleep_time = float(retry_after) if retry_after else retry_time\n",
    "#                 print(f\"Rate limited (429) for location {location_id}. Sleeping for {sleep_time} seconds before retrying...\")\n",
    "#                 time.sleep(sleep_time)\n",
    "#                 retry_time *= 2\n",
    "#                 continue\n",
    "#             elif response.status_code == 401:\n",
    "#                 print(f\"Unauthorized (401) for location {location_id}. Check your API key and header format.\")\n",
    "#                 break\n",
    "#             else:\n",
    "#                 attempt += 1\n",
    "#                 print(f\"HTTP error for location {location_id} on attempt {attempt}: {http_err}. Retrying in {retry_time} seconds...\")\n",
    "#                 time.sleep(retry_time)\n",
    "#                 retry_time *= 2\n",
    "\n",
    "#         except Exception as e:\n",
    "#             attempt += 1\n",
    "#             print(f\"Error for location {location_id} on attempt {attempt}: {e}. Retrying in {retry_time} seconds...\")\n",
    "#             time.sleep(retry_time)\n",
    "#             retry_time *= 2\n",
    "\n",
    "#     if not success:\n",
    "#         print(f\"Failed to fetch data for location {location_id} after {max_attempts} attempts.\")\n",
    "\n",
    "#     # Pause between calls for a random duration between 1 and 2 seconds\n",
    "#     pause = random.uniform(1, 2)\n",
    "#     print(f\"Pausing for {pause:.2f} seconds before the next call...\")\n",
    "#     time.sleep(pause)\n",
    "\n",
    "# # Initialize the Spark session\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# # Convert the list of dictionaries to a Spark DataFrame.\n",
    "# # With \"results\" now being a string, Spark can properly infer the schema.\n",
    "# latest_sensor_df = spark.createDataFrame(latest_sensor_data)\n",
    "\n",
    "# # Optionally inspect the DataFrame\n",
    "# # latest_sensor_df.printSchema()\n",
    "# # latest_sensor_df.show(10)\n",
    "\n",
    "# # Save the raw data (as JSON, preserving nested structures) into the raw table\n",
    "# raw_table_name = \"sensor_measurements_raw\"\n",
    "\n",
    "# latest_sensor_df.write.mode(\"append\").format(\"delta\").saveAsTable(raw_table_name)\n",
    "\n",
    "# print(f\"Raw sensor data successfully saved to the table: {raw_table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9914c20-68ea-4809-9a39-514e42734078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Addind limiters, so I can call 5 days of data without exceeding limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05de46ab-0768-464f-9f19-e7ee3d475f61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1cbec70-8e3b-4025-bdcf-2a3fe97227c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, per_minute=60, per_hour=2000):\n",
    "        self.per_minute_calls = deque(maxlen=per_minute)\n",
    "        self.per_hour_calls = deque(maxlen=per_hour)\n",
    "        self.per_minute_limit = per_minute\n",
    "        self.per_hour_limit = per_hour\n",
    "\n",
    "    def wait_if_needed(self):\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        # Clean up old timestamps\n",
    "        while self.per_minute_calls and current_time - self.per_minute_calls[0] > timedelta(minutes=1):\n",
    "            self.per_minute_calls.popleft()\n",
    "        while self.per_hour_calls and current_time - self.per_hour_calls[0] > timedelta(hours=1):\n",
    "            self.per_hour_calls.popleft()\n",
    "        \n",
    "        # Check if we need to wait\n",
    "        if len(self.per_minute_calls) >= self.per_minute_limit:\n",
    "            sleep_time = 60 - (current_time - self.per_minute_calls[0]).total_seconds()\n",
    "            if sleep_time > 0:\n",
    "                logger.info(f\"Rate limit approaching, waiting {sleep_time:.2f} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "        \n",
    "        if len(self.per_hour_calls) >= self.per_hour_limit:\n",
    "            sleep_time = 3600 - (current_time - self.per_hour_calls[0]).total_seconds()\n",
    "            if sleep_time > 0:\n",
    "                logger.info(f\"Hourly limit approaching, waiting {sleep_time:.2f} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "        \n",
    "        # Record this call\n",
    "        current_time = datetime.now()\n",
    "        self.per_minute_calls.append(current_time)\n",
    "        self.per_hour_calls.append(current_time)\n",
    "\n",
    "def fetch_location_data(location_id, api_key, base_url, max_attempts=5):\n",
    "    url = f\"{base_url}/locations/{location_id}/latest\"\n",
    "    headers = {\"X-API-Key\": api_key}\n",
    "    \n",
    "    attempt = 0\n",
    "    retry_time = 0.5\n",
    "    \n",
    "    while attempt < max_attempts:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            # Convert the nested \"results\" field to a JSON string\n",
    "            if \"results\" in data:\n",
    "                data[\"results\"] = json.dumps(data[\"results\"])\n",
    "\n",
    "            # Add location_id if not present\n",
    "            if \"location_id\" not in data:\n",
    "                data[\"location_id\"] = location_id\n",
    "\n",
    "            logger.info(f\"Successfully fetched data for location {location_id}\")\n",
    "            return data\n",
    "\n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            if response.status_code == 429:\n",
    "                retry_after = response.headers.get(\"Retry-After\")\n",
    "                sleep_time = float(retry_after) if retry_after else retry_time\n",
    "                logger.warning(f\"Rate limited (429) for location {location_id}. Sleeping for {sleep_time} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "                retry_time *= 2\n",
    "                continue\n",
    "            elif response.status_code == 401:\n",
    "                logger.error(f\"Unauthorized (401) for location {location_id}. Check API key.\")\n",
    "                return None\n",
    "            else:\n",
    "                attempt += 1\n",
    "                logger.warning(f\"HTTP error for location {location_id} on attempt {attempt}: {http_err}\")\n",
    "                time.sleep(retry_time)\n",
    "                retry_time *= 2\n",
    "\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            logger.error(f\"Error for location {location_id} on attempt {attempt}: {e}\")\n",
    "            time.sleep(retry_time)\n",
    "            retry_time *= 2\n",
    "\n",
    "    logger.error(f\"Failed to fetch data for location {location_id} after {max_attempts} attempts.\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    # API Configuration\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    api_key = os.getenv('OPENAQ_API_KEY')\n",
    "\n",
    "\n",
    "    API_KEY = api_key\n",
    "    BASE_URL = \"https://api.openaq.org/v3\"\n",
    "\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Load imported df of locations, acquired from the FIRMS import \n",
    "    distinct_sensors_df = spark.table(\"temp_locations_for_api\")\n",
    "\n",
    "    # Get location IDs\n",
    "    location_ids = distinct_sensors_df.select(\"location_id\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    # Initialize rate limiter\n",
    "    rate_limiter = RateLimiter(per_minute=60, per_hour=2000)\n",
    "\n",
    "    # Process locations in batches\n",
    "    batch_size = 55  # Slightly under the per-minute limit\n",
    "    location_batches = [location_ids[i:i + batch_size] for i in range(0, len(location_ids), batch_size)]\n",
    "    \n",
    "    latest_sensor_data = []\n",
    "\n",
    "    # Process each batch with progress bar\n",
    "    for batch in tqdm(location_batches, desc=\"Processing location batches\"):\n",
    "        batch_data = []\n",
    "        for location_id in batch:\n",
    "            rate_limiter.wait_if_needed()\n",
    "            data = fetch_location_data(location_id, API_KEY, BASE_URL)\n",
    "            if data:\n",
    "                batch_data.append(data)\n",
    "\n",
    "        latest_sensor_data.extend(batch_data)\n",
    "        \n",
    "        # Optional: Save batch data periodically\n",
    "        if len(latest_sensor_data) >= 500:  # Adjust threshold as needed\n",
    "            temp_df = spark.createDataFrame(latest_sensor_data)\n",
    "            temp_df.write.mode(\"append\").format(\"delta\").saveAsTable(\"sensor_measurements_raw\")\n",
    "            latest_sensor_data = []\n",
    "            logger.info(\"Saved batch to Delta table\")\n",
    "\n",
    "    # Save any remaining data\n",
    "    if latest_sensor_data:\n",
    "        final_df = spark.createDataFrame(latest_sensor_data)\n",
    "        final_df.write.mode(\"append\").format(\"delta\").saveAsTable(\"sensor_measurements_raw\")\n",
    "        logger.info(\"Saved final batch to Delta table\")\n",
    "\n",
    "    logger.info(\"Data collection complete\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze 2 - Air Quality Pull",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
