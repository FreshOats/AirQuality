{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6fb52e4-b88b-4c8a-8ef0-2cc165ce6462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This takes the Bronze Layer Acquisition of the OpenAQ data for the sensors from the cumulative table, validates that the table is idempotent as the Audit part of the process. Then this information is passed forward to analytics. \n",
    "\n",
    "In the future, there will be an additional state change table that will indicate whether or not to continue to monitor certain sensors, which will, in turn, regulate which sensors are called upon for the next API call. \n",
    "\n",
    "Once this data is saved to volume, it will be sent to analytics. In the future, this will be ingested into a Snowflake Data Warehouse, but for now, I'm going to set up the job in Databricks and have the analytics also shown in Databricks as the final point of the process. The analytics queries will be performed in the Gold Layer as they are prepared for and sent to analytics visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c29b927-71ac-4773-a576-25cddb8d6482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = spark.table(\"sensor_measurements_raw\")\n",
    "display(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398d1764-1af5-40bc-98e0-23ab4a886122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n",
    "\n",
    "# Define the schema for each element in the results array.\n",
    "results_element_schema = StructType([\n",
    "    StructField(\"datetime\", StructType([\n",
    "        StructField(\"utc\", StringType()),\n",
    "        StructField(\"local\", StringType())\n",
    "    ])),\n",
    "    StructField(\"value\", DoubleType()),\n",
    "    StructField(\"coordinates\", StructType([\n",
    "        StructField(\"latitude\", DoubleType()),\n",
    "        StructField(\"longitude\", DoubleType())\n",
    "    ])),\n",
    "    StructField(\"sensorsId\", IntegerType()),\n",
    "    StructField(\"locationsId\", IntegerType())\n",
    "])\n",
    "# Wrap the element schema in an ArrayType.\n",
    "results_schema = ArrayType(results_element_schema)\n",
    "\n",
    "# Assuming raw_df is loaded from your sensor_measurements_raw table.\n",
    "raw_df = spark.table(\"sensor_measurements_raw\")\n",
    "\n",
    "# For the 'results' column, if it's not already an array type, cast to string and parse.\n",
    "df_parsed = raw_df.withColumn(\n",
    "    \"results_json\",\n",
    "    from_json( col(\"results\").cast(\"string\"), results_schema )\n",
    ")\n",
    "\n",
    "# Explode the results array to get one row per measurement.\n",
    "df_exploded = df_parsed.withColumn(\"result\", explode(col(\"results_json\")))\n",
    "\n",
    "# Flatten the DataFrame.\n",
    "# Since meta is a map, we can access its value directly using [\"key\"].\n",
    "df_flattened = df_exploded.select(\n",
    "    col(\"location_id\"),\n",
    "    col(\"meta\")[\"website\"].alias(\"website\"),\n",
    "    col(\"meta\")[\"name\"].alias(\"api_name\"),\n",
    "    col(\"meta\")[\"page\"].alias(\"page\"),\n",
    "    col(\"meta\")[\"found\"].alias(\"found\"),\n",
    "    col(\"meta\")[\"limit\"].alias(\"limit\"),\n",
    "    col(\"result.datetime.utc\").alias(\"datetime\"),\n",
    "    col(\"result.datetime.local\").alias(\"local_datetime\"),\n",
    "    col(\"result.value\").alias(\"value\"),\n",
    "    col(\"result.coordinates.latitude\").alias(\"lat\"),\n",
    "    col(\"result.coordinates.longitude\").alias(\"lon\"),\n",
    "    col(\"result.sensorsId\").alias(\"sensor_id\"),\n",
    "    col(\"result.locationsId\").alias(\"result_location_id\")\n",
    ")\n",
    "\n",
    "# Optionally, display the result.\n",
    "# df_flattened.printSchema()\n",
    "# df_flattened.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd86e27f-1839-4f6a-98c7-94a19c029749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31850ae2-2e11-4f86-bb9a-16d7664f3ac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "filtered_sensors = spark.table(\"filtered_sensors\")\n",
    "\n",
    "df_filtered = df_flattened.where(\"datetime >='2025-01-01'\") \\\n",
    "        .join(filtered_sensors, on=[\"location_id\", \"sensor_id\"], how=\"inner\") \\\n",
    "            .withColumnRenamed(\"parameter_name\", \"parameter\") \\\n",
    "            .withColumnRenamed(\"parameter_units\", \"units\") \\\n",
    "            .select(\"location_id\", \"sensor_id\", \"parameter\", \"units\", \"value\", \"datetime\", \"lat\", \"lon\") \\\n",
    "            .dropDuplicates([\"location_id\", \"sensor_id\", \"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b16817-5e0e-4012-89a1-c1c24e239b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d469d0d4-7c12-4066-a4ac-68d3d23fd696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "permanent_df = spark.table(\"permanent_sensor_measurements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461e95fa-dd0a-4c10-9e0c-3ec19586f6fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(permanent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8a2f430-7be2-442b-b8b7-0de1066fd897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "duplicate_check_df = df_filtered.groupBy(\"location_id\", \"sensor_id\", \"datetime\").count().filter(\"count > 1\")\n",
    "\n",
    "if duplicate_check_df.count() > 0:\n",
    "    print(\"Duplicates detected in the enriched data:\")\n",
    "    duplicate_check_df.show()\n",
    "else:\n",
    "    print(\"No duplicates found. Data is idempotent based on the common key and timestamp.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "601d0433-fd6f-4e19-b647-f1968ce24e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Merge the enriched raw data into the permenant table\n",
    "deltaTable = DeltaTable.forName(spark, \"permanent_sensor_measurements\")\n",
    "\n",
    "deltaTable.alias(\"target\").merge(\n",
    "    source=df_filtered.alias(\"source\"),\n",
    "    condition=\"\"\"\n",
    "      target.location_id = source.location_id AND\n",
    "      target.sensor_id = source.sensor_id AND\n",
    "      target.datetime = source.datetime\n",
    "    \"\"\"\n",
    "  ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "print(\"Merge complete. Permanent table updated idempotently.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb76692e-4047-4130-8b2e-599d914e53a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM permanent_sensor_measurements\n",
    "WHERE to_date(datetime) = current_date() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1fe0728-fe5c-4790-bc87-9c133a90428f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "-- CREATE TABLE daily_air_quality_monitor\n",
    "-- USING DELTA \n",
    "-- PARTITIONED BY (result_date) AS\n",
    "\n",
    "\n",
    "MERGE INTO daily_air_quality_monitor target\n",
    "USING (\n",
    "    WITH evaluated_measurements AS (\n",
    "      SELECT \n",
    "        m.location_id,\n",
    "        m.sensor_id,\n",
    "        m.parameter,\n",
    "        m.units,\n",
    "        m.value,\n",
    "        to_date(m.datetime) AS result_date,\n",
    "        m.lat,\n",
    "        m.lon,\n",
    "        CASE \n",
    "          WHEN m.value >= r.Hazardous THEN 'Hazardous'\n",
    "          WHEN m.value >= r.Severe THEN 'Severe'\n",
    "          WHEN m.value >= r.Moderate THEN 'Moderate'\n",
    "          ELSE 'Good'\n",
    "        END AS hazard_level\n",
    "      FROM permanent_sensor_measurements m\n",
    "      LEFT JOIN air_quality_reference_parameters r\n",
    "        ON m.parameter = r.Parameter\n",
    "    ), \n",
    "    health_hazard_levels AS (\n",
    "    SELECT * \n",
    "    FROM evaluated_measurements\n",
    "    WHERE hazard_level IN ('Hazardous', 'Severe', 'Moderate') \n",
    "      AND result_date = current_date()\n",
    "      AND parameter NOT IN ('temperature', 'um003', 'pm1', 'relativehumidity')\n",
    "    ) \n",
    "    SELECT \n",
    "      s.GEOID\n",
    "      , s.state\n",
    "      , h.parameter\n",
    "      , h.units\n",
    "      , h.value\n",
    "      , h.hazard_level\n",
    "      , s.median_income\n",
    "      , s.median_income_margin\n",
    "      , s.income_bracket\n",
    "      , h.result_date\n",
    "    FROM health_hazard_levels h\n",
    "    INNER JOIN sensors_with_income_levels s\n",
    "      ON h.location_id = s.nn_location_id AND h.sensor_id = s.sensor_id\n",
    ") source\n",
    "ON target.GEOID = source.GEOID \n",
    "  AND target.parameter = source.parameter \n",
    "  AND target.value = source.value \n",
    "  AND target.result_date = source.result_date\n",
    "WHEN NOT MATCHED THEN \n",
    "  INSERT (GEOID, state, parameter, units, value, hazard_level, \n",
    "          median_income, median_income_margin, income_bracket, result_date)\n",
    "  VALUES (source.GEOID, source.state, source.parameter, source.units, source.value, \n",
    "          source.hazard_level, source.median_income, source.median_income_margin, \n",
    "          source.income_bracket, source.result_date);\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3905852250677815,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver Layer 2 - Air Quality Final Transform and Census Merge",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
