{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e4cd573-cbf1-46e4-a17c-02f7c314af17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Extract and Collect TIGERS/ACS Files\n",
    "\n",
    "The TIGERS files have income included in the 2021 5 year estimate, which substantially reduces the amount of compute necessary in this pipeline. Each state and territory has its own file. The granularity pulled is Census Tract level with the associated geometric files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f3d0cf-f8c2-4dda-a0f7-c2e7f6bbf56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create new spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Geometric_Dataset_Processing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f5fd83e-91c2-4f7d-bccb-8ae827da17ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Define the source directory containing the .gdb.zip files\n",
    "source_dir = \"/Volumes/tabular/dataexpert/freshoats_capstone/ACS_Tract_Zipped/\"  \n",
    "\n",
    "# Define the target directory where the unzipped contents will be placed\n",
    "target_dir = \"/Volumes/tabular/dataexpert/freshoats_capstone/ACS_Geo_Unzipped/\"  \n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through all .zip files in the source directory\n",
    "for file_name in os.listdir(source_dir):\n",
    "    if file_name.endswith(\".zip\"):  # Check if the file is a .zip file\n",
    "        zip_path = os.path.join(source_dir, file_name)  # Full path to the .zip file\n",
    "        \n",
    "        # Extract the contents of the .zip file directly into the target directory\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            for member in zip_ref.namelist():\n",
    "                # Extract each file/folder to the target directory\n",
    "                zip_ref.extract(member, target_dir)\n",
    "        \n",
    "        print(f\"Extracted {file_name} to {target_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f00b0855-5915-4bd9-a968-620dc0942823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install geopandas pyogrio fiona\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8434f1c-57b0-4b48-8d74-464716dea2d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa406a8-221c-4141-81d1-28e45507bf3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pyogrio\n",
    "\n",
    "# Path to the .gdb file\n",
    "gdb_path = \"/Volumes/tabular/dataexpert/freshoats_capstone/ACS_Geo_Unzipped/ACS_2021_5YR_TRACT_01_ALABAMA.gdb/\"\n",
    "\n",
    "# List all layers in the geodatabase\n",
    "layers = pyogrio.list_layers(gdb_path)\n",
    "print(\"Available layers:\", layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b89d9e5-c5af-4260-8885-50358f867e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the spatial layer\n",
    "spatial_layer = \"ACS_2021_5YR_TRACT_01_ALABAMA\"\n",
    "gdf = gpd.read_file(gdb_path, layer=spatial_layer)\n",
    "\n",
    "# Show the first few rows of the GeoDataFrame\n",
    "print(gdf.head())\n",
    "\n",
    "# Check the columns\n",
    "print(gdf.columns)\n",
    "\n",
    "# Check the GEOID format\n",
    "print(gdf.GEOID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c8801d3-34c3-4ae6-b577-04cac5c0ea4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select the needed columns from spatial data\n",
    "spatial_gdf = gdf[['GEOID_Data', 'STATEFP', 'COUNTYFP', 'TRACTCE', 'INTPTLAT', 'INTPTLON', 'geometry']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29c0eb4f-3f22-4692-8323-a0e3ecbd620e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spatial_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f112238-ec36-43a6-9d35-1e382c2e555a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514a6750-229a-4e88-b6fc-db20d9fd6caf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the attribute layer (e.g., income data)\n",
    "attribute_layer = \"X19_INCOME\"\n",
    "attribute_gdf = gpd.read_file(gdb_path, layer=attribute_layer)\n",
    "\n",
    "# Show the first few rows of the attribute GeoDataFrame\n",
    "print(attribute_gdf.head())\n",
    "\n",
    "# Check the columns\n",
    "print(attribute_gdf.columns)\n",
    "\n",
    "attribute_gdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17e8b7ab-1ea2-4272-aff3-af2ba0954937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notice there are 3045 columns here. They break up all of the information by different demographics that I'm not interested. I just want the median income of everyone living in the tract. We need to find this with the metadata controlling for any race being false. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d961f43-c3e6-4244-a788-6691573c298b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The column titles are completely unusable at the moment, and most are likely unnecessary. To get the titles for each column, I need to extract the information from the Tract metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c64a80f3-37e8-4898-bf80-f38770587899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the metadata layer\n",
    "metadata_layer = \"TRACT_METADATA_2021\"\n",
    "metadata_gdf = gpd.read_file(gdb_path, layer=metadata_layer)\n",
    "\n",
    "# Display the first few rows of the metadata layer\n",
    "print(metadata_gdf.head())\n",
    "\n",
    "# Check the columns in the metadata layer\n",
    "print(metadata_gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baf63258-f7db-4310-8607-e609f108f30b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metadata_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa49617-f44a-4f7f-879f-a8ac63d024d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Search for columns related to \"median\"\n",
    "median_columns = metadata_gdf[metadata_gdf[\"Full_Name\"].str.contains(\"median\", case=False, na=False)]\n",
    "\n",
    "# Search for columns related to \"per capita\"\n",
    "per_capita_columns = metadata_gdf[metadata_gdf[\"Full_Name\"].str.contains(\"per capita\", case=False, na=False)]\n",
    "\n",
    "# Search for columns related to \"mean\"\n",
    "mean_columns = metadata_gdf[metadata_gdf[\"Full_Name\"].str.contains(\"mean\", case=False, na=False)]\n",
    "\n",
    "# Display the results\n",
    "print(\"Median Columns:\")\n",
    "print(median_columns)\n",
    "\n",
    "print(\"\\nPer Capita Columns:\")\n",
    "print(per_capita_columns)\n",
    "\n",
    "print(\"\\nMean Columns:\")\n",
    "print(mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e656e9b-2bb3-47db-9675-9cfedcbe1fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)  # Show full column content\n",
    "\n",
    "# Filter metadata for \"Median Household Income\" and exclude race-related rows\n",
    "overall_median_income = metadata_gdf[\n",
    "    metadata_gdf[\"Full_Name\"].str.contains(\"Median Household Income\", case=False, na=False) &\n",
    "    ~metadata_gdf[\"Full_Name\"].str.contains(\"race|ethnicity|hispanic|white|black|asian|native\", case=False, na=False)\n",
    "]\n",
    "\n",
    "# Display the filtered rows\n",
    "print(overall_median_income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3d87123-5d56-410c-ab70-fdd048c78b4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I've Identified the columns to use: \n",
    "Estimate Column: **B19013e1**\n",
    "Description: \"Median Household Income in the Past 12 Months (in 2021 Inflation-Adjusted Dollars): Households -- (Estimate)\"\n",
    "This column provides the overall median household income for all households.\n",
    "Margin of Error Column: **B19013m1**\n",
    "Description: \"Median Household Income in the Past 12 Months (in 2021 Inflation-Adjusted Dollars): Households -- (Margin of Error)\"\n",
    "This column provides the margin of error for the overall median household income estimate.\n",
    "\n",
    "I'm going select these columns and then rename them to be more meaningful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a76d0296-265d-4130-9a32-1f71434868a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select relevant columns from the income table\n",
    "filtered_attribute_gdf = attribute_gdf[[\"GEOID\", \"B19013e1\", \"B19013m1\"]]\n",
    "\n",
    "# Show the filtered income table\n",
    "filtered_attribute_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c1860a-d3c0-4f4e-b9e1-a699e41bf0b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename columns in PySpark DataFrame\n",
    "filtered_attribute_gdf = filtered_attribute_gdf.rename(columns={\"B19013e1\": \"median_income\", \"B19013m1\": \"median_income_margin\"})\n",
    "\n",
    "filtered_attribute_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc8d3c7-f6c8-4a25-8ef6-dc47c22fb9ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge the spatial layer and the X19_INCOME layer on the appropriate columns\n",
    "merged_gdf = gdf.merge(filtered_attribute_gdf, left_on=\"GEOID_Data\", right_on=\"GEOID\")\n",
    "\n",
    "# Verify the merged GeoDataFrame\n",
    "merged_gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d719d0d-f279-45d6-ae4f-bed9d42b3684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(merged_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a3b5382-10b0-4edf-a00c-dba7f4d0035c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_gdf = merged_gdf[[\"GEOID_Data\", \"median_income\", \"median_income_margin\", \"geometry\", \"STATEFP\", \"TRACTCE\", \"INTPTLAT\", \"INTPTLON\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55985ddd-bfcc-42c3-99d8-d3f1ff890920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_gdf.to_parquet(\"/Volumes/tabular/dataexpert/freshoats_capstone/Geo_Census.parquet\", engine=\"pyarrow\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ff5fa4-4251-488e-bdb1-7fac9d7e4887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/Volumes/tabular/dataexpert/freshoats_capstone/Geo_Census.parquet\")\n",
    "\n",
    "/Volumes/tabular/dataexpert/freshoats_capstone/ACS_Geo_Unzipped/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e574d300-7f72-450c-8d05-44d291f9f3ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "That worked - no we need to iterate through the rest of the files to perform the same transformation to all states and territories and append them to the Geo_Census.parquet file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1c95d1b-0897-4049-90a4-216a7a9f9cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install geopandas pyarrow pyogrio fiona\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75f6a8f7-3d56-4267-930d-2c147329bda7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f1f3af-be4e-4785-8349-65943cba013d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Input folder containing the files\n",
    "input_folder = \"/Volumes/tabular/dataexpert/freshoats_capstone/ACS_Geo_Unzipped/\"\n",
    "\n",
    "# Output Parquet file\n",
    "output_file = \"/Volumes/tabular/dataexpert/freshoats_capstone/Geo_Census.parquet\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a66a8f0-f1d8-455a-af47-c7ab2e9114fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_gdb(gdb_path, output_file):\n",
    "    try:\n",
    "        # Extract the state name from the folder name\n",
    "        state_name = os.path.basename(gdb_path).replace(\".gdb\", \"\")\n",
    "        print(f\"Processing: {state_name}\")\n",
    "\n",
    "        # Step 1a: Load the spatial layer\n",
    "        spatial_layer = state_name  # The folder name is the spatial layer name\n",
    "        gdf = gpd.read_file(gdb_path, layer=spatial_layer)\n",
    "\n",
    "        # Step 1b: Select the needed columns from spatial data\n",
    "        spatial_gdf = gdf[['GEOID_Data', 'STATEFP', 'COUNTYFP', 'TRACTCE', 'INTPTLAT', 'INTPTLON', 'geometry']]\n",
    "\n",
    "        # Step 2: Load the attribute layer (e.g., income data)\n",
    "        attribute_layer = \"X19_INCOME\"\n",
    "        attribute_gdf = gpd.read_file(gdb_path, layer=attribute_layer)\n",
    "\n",
    "        # Step 3: Select relevant columns from the income table\n",
    "        filtered_attribute_gdf = attribute_gdf[[\"GEOID\", \"B19013e1\", \"B19013m1\"]]\n",
    "\n",
    "        # Step 4: Rename columns\n",
    "        filtered_attribute_gdf = filtered_attribute_gdf.rename(columns={\n",
    "            \"B19013e1\": \"median_income\",\n",
    "            \"B19013m1\": \"median_income_margin\"\n",
    "        })\n",
    "\n",
    "        # Step 5: Merge the spatial layer and the X19_INCOME layer\n",
    "        merged_gdf = spatial_gdf.merge(filtered_attribute_gdf, left_on=\"GEOID_Data\", right_on=\"GEOID\")\n",
    "\n",
    "        # Step 6: Handle appending to the Parquet file\n",
    "        if os.path.exists(output_file):\n",
    "            # Read the existing Parquet file\n",
    "            existing_gdf = gpd.read_parquet(output_file)\n",
    "\n",
    "            # Concatenate the new data with the existing data\n",
    "            combined_gdf = pd.concat([existing_gdf, merged_gdf], ignore_index=True)\n",
    "\n",
    "            # Overwrite the Parquet file with the combined data\n",
    "            combined_gdf.to_parquet(output_file, engine=\"pyarrow\", index=False)\n",
    "        else:\n",
    "            # Save as a new Parquet file\n",
    "            merged_gdf.to_parquet(output_file, engine=\"pyarrow\", index=False)\n",
    "\n",
    "        print(f\"Successfully processed and appended: {state_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gdb_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57619585-235f-4882-8e3c-4619f47344d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Iterate through all .gdb folders in the input directory\n",
    "for folder_name in os.listdir(input_folder):\n",
    "    folder_path = os.path.join(input_folder, folder_name)\n",
    "\n",
    "    # Check if the folder is a .gdb folder\n",
    "    if folder_name.endswith(\".gdb\"):\n",
    "        process_gdb(folder_path, output_file)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Static Layer 1 - TIGERS Geodata Census Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
