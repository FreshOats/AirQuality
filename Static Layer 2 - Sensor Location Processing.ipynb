{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03318f09-8d7d-4fd3-b719-4e6b635d0fcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Sensor Locations were acquired from OpenAQ locally and the csv was imported into databricks using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0941f13-020e-4a1f-83a1-24082869e664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install openaq, python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a69b7b1f-f6bc-4fca-b8d3-93f73d66e40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openaq import OpenAQ\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "api_key = os.getenv('OPENAQ_API_KEY')\n",
    "\n",
    "client = OpenAQ(api_key=api_key)\n",
    "\n",
    "try:\n",
    "    # Make a single request to fetch the first page of locations in the US\n",
    "    response = client.locations.list(iso=\"US\", limit=2)  # Fetch 10 results\n",
    "    locations = response.results  # Get the list of locations\n",
    "\n",
    "    # Inspect the first location object\n",
    "    for location in locations:\n",
    "        print(location)  # Print the location object to inspect its structure\n",
    "\n",
    "finally:\n",
    "    # Ensure the client is properly closed\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bad9ff4-a20e-4eb2-b777-07f31de4323a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15247fae-9c4e-4d99-9a6f-61cfbd2d1a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openaq import OpenAQ\n",
    "import csv\n",
    "from math import ceil\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "api_key = os.getenv('OPENAQ_API_KEY')\n",
    "\n",
    "client = OpenAQ(api_key=api_key)\n",
    "\n",
    "output_file = \"/dbfs/FileStore/us_sensor_locations.csv\"\n",
    "\n",
    "try:\n",
    "    # Define the CSV header with fields you want to capture.\n",
    "    fieldnames = [\n",
    "        \"location_id\", \"name\", \"locality\", \"timezone\",\n",
    "        \"country_id\", \"country_code\", \"country_name\",\n",
    "        \"owner_id\", \"owner_name\",\n",
    "        \"provider_id\", \"provider_name\",\n",
    "        \"is_mobile\", \"is_monitor\",\n",
    "        \"instruments\", \"sensors\",\n",
    "        \"latitude\", \"longitude\", \"bounds\",\n",
    "        \"distance\", \"datetime_first\", \"datetime_last\"\n",
    "    ]\n",
    "    \n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Start pagination (you can also compute total pages if desired)\n",
    "        page = 1\n",
    "        limit = 1000\n",
    "\n",
    "        while True:\n",
    "            print(f\"Fetching page {page}...\")\n",
    "            response = client.locations.list(iso=\"US\", limit=limit, page=page)\n",
    "            locations = response.results\n",
    "\n",
    "            if not locations:\n",
    "                break\n",
    "\n",
    "            for location in locations:\n",
    "                # Skip locations without coordinates (or handle them as you wish)\n",
    "                if not location.coordinates:\n",
    "                    continue\n",
    "\n",
    "                # Flatten nested objects.\n",
    "                country = location.country if hasattr(location, \"country\") else None\n",
    "                owner = location.owner if hasattr(location, \"owner\") else None\n",
    "                provider = location.provider if hasattr(location, \"provider\") else None\n",
    "\n",
    "                # For lists like instruments and sensors, join the names (or any field) with a separator.\n",
    "                instruments = (\n",
    "                    \"; \".join([inst.name for inst in location.instruments])\n",
    "                    if location.instruments else \"\"\n",
    "                )\n",
    "                # For sensors, you might also want to include parameter details.\n",
    "                sensors = \"\"\n",
    "                if location.sensors:\n",
    "                    sensors_list = []\n",
    "                    for sensor in location.sensors:\n",
    "                        param = sensor.parameter if hasattr(sensor, \"parameter\") else sensor.get(\"parameter\", {})\n",
    "                        sensor_str = f\"{sensor.id}:{sensor.name} ({param.name if hasattr(param, 'name') else param.get('name', '')})\"\n",
    "                        sensors_list.append(sensor_str)\n",
    "                    sensors = \"; \".join(sensors_list)\n",
    "\n",
    "                writer.writerow({\n",
    "                    \"location_id\": location.id,\n",
    "                    \"name\": location.name,\n",
    "                    \"locality\": location.locality,\n",
    "                    \"timezone\": location.timezone,\n",
    "                    \"country_id\": country.id if country and hasattr(country, \"id\") else (country.get(\"id\") if country else None),\n",
    "                    \"country_code\": country.code if country and hasattr(country, \"code\") else (country.get(\"code\") if country else None),\n",
    "                    \"country_name\": country.name if country and hasattr(country, \"name\") else (country.get(\"name\") if country else None),\n",
    "                    \"owner_id\": owner.id if owner and hasattr(owner, \"id\") else (owner.get(\"id\") if owner else None),\n",
    "                    \"owner_name\": owner.name if owner and hasattr(owner, \"name\") else (owner.get(\"name\") if owner else None),\n",
    "                    \"provider_id\": provider.id if provider and hasattr(provider, \"id\") else (provider.get(\"id\") if provider else None),\n",
    "                    \"provider_name\": provider.name if provider and hasattr(provider, \"name\") else (provider.get(\"name\") if provider else None),\n",
    "                    \"is_mobile\": location.is_mobile,\n",
    "                    \"is_monitor\": location.is_monitor,\n",
    "                    \"instruments\": instruments,\n",
    "                    \"sensors\": sensors,\n",
    "                    \"latitude\": location.coordinates.latitude,\n",
    "                    \"longitude\": location.coordinates.longitude,\n",
    "                    \"bounds\": \", \".join(str(b) for b in location.bounds) if location.bounds else \"\",\n",
    "                    \"distance\": location.distance,\n",
    "                    \"datetime_first\": location.datetime_first,\n",
    "                    \"datetime_last\": location.datetime_last\n",
    "                })\n",
    "            page += 1\n",
    "\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32448225-3fdd-48b1-acc5-4255fa3defd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sensor_locations_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"dbfs:/FileStore/AirQuality/us_all_location_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aeafcc5-d621-462a-b6b5-37e919c38016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sensor_locations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37904e1d-0ddf-4fac-8094-2755e8a0d0e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate Coordinates are within the U.S.\n",
    "# Define the bounding box for the U.S.\n",
    "min_lat, max_lat = 18.0, 71.538800\n",
    "min_lon, max_lon = -179.148909, -66.93457\n",
    "\n",
    "# Filter the DataFrame\n",
    "valid_locations_df = sensor_locations_df.filter(\n",
    "    (sensor_locations_df[\"latitude\"] >= min_lat) &\n",
    "    (sensor_locations_df[\"latitude\"] <= max_lat) &\n",
    "    (sensor_locations_df[\"longitude\"] >= min_lon) &\n",
    "    (sensor_locations_df[\"longitude\"] <= max_lon)\n",
    ")\n",
    "\n",
    "# Validate that all sensor locations are in the United States including Alaska and Hawaii\n",
    "valid_locations_df.count() == sensor_locations_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a14d9f-7e45-417f-b01f-5f6320b32fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the bounding box excluding HI and AK\n",
    "min_lat, max_lat =  24.396308, 49.384358\n",
    "min_lon, max_lon = -125.0, -66.93457\n",
    "\n",
    "# Filter the DataFrame\n",
    "valid_locations_c_df = sensor_locations_df.filter(\n",
    "    (sensor_locations_df[\"latitude\"] >= min_lat) &\n",
    "    (sensor_locations_df[\"latitude\"] <= max_lat) &\n",
    "    (sensor_locations_df[\"longitude\"] >= min_lon) &\n",
    "    (sensor_locations_df[\"longitude\"] <= max_lon)\n",
    ")\n",
    "\n",
    "# Validate that all sensor locations are in the United States excluding Alaska and Hawaii\n",
    "valid_locations_c_df.count() == sensor_locations_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ab137c4-cd59-4ffc-8d04-834dba95d385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This verifies that there are sensors included outside of the continental US, which is a good thing!\n",
    "\n",
    "Now Check for duplicates and Dedupe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a16ec9-995e-4c22-8928-fec0c048b174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by all columns and count occurrences\n",
    "duplicates_df = sensor_locations_df.groupBy(sensor_locations_df.columns).count()\n",
    "\n",
    "# Filter rows where count > 1 (indicating duplicates)\n",
    "duplicates_df = duplicates_df.filter(duplicates_df[\"count\"] > 1)\n",
    "\n",
    "# Show duplicate rows\n",
    "duplicates_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a5fb481-01c7-4948-aa45-74eb103d9e11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows\n",
    "duplicate_count = sensor_locations_df.groupBy(sensor_locations_df.columns).count().filter(\"count > 1\").count()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d72bffd-17b4-4937-98fb-9826591463b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Drop the columns that won't be used, especially those that don't need to be flattened:\n",
    "\n",
    "location_id\t\n",
    "name\t\n",
    "locality\t\n",
    "timezone\t\n",
    "country_id\t\n",
    "country_code\t\n",
    "country_name\t\n",
    "owner_id\t\n",
    "owner_name\t\n",
    "provider_id\t\n",
    "provider_name\t\n",
    "is_mobile\t\n",
    "is_monitor\t\n",
    "instruments\t\n",
    "sensors\t\n",
    "latitude\t\n",
    "longitude\t\n",
    "bounds\t\n",
    "distance\t\n",
    "datetime_first\t\n",
    "datetime_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94ed5503-ec34-40a3-a8b6-f58150a99b2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sensor_locations_df = sensor_locations_df.select(\n",
    "    \"location_id\"\t\t\n",
    "    , \"sensors\"\t\n",
    "    , \"latitude\"\t\n",
    "    , \"longitude\"\t\t\n",
    "    , \"datetime_last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdae59b5-4abf-4cf9-82b1-f581cc111789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Flatten the Sensors and datetime_last columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6afabd6c-2a76-4033-873c-f6fdc2a4ec59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col, to_date, lit\n",
    "\n",
    "# Convert the datetimeLast column from having both utc and local in a complex string to just date in utc\n",
    "sensor_locations_df = sensor_locations_df.withColumn(\"datetime_last\", to_date(regexp_extract(\"datetime_last\", r\"utc='(.*?)'\", 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca9b679-2ab2-4d19-82d7-80d2d62ab744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sensor_locations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6049c04-abb5-453f-bbea-a7f7365dac1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, trim, regexp_extract\n",
    "\n",
    "# Assume sensor_locations_df is your existing DataFrame with:\n",
    "# location_id, sensors, latitude, longitude, datetime_last\n",
    "\n",
    "# Step 1: Split the sensors string by semicolon to create an array, then explode it.\n",
    "df = sensor_locations_df.withColumn(\"sensor_list\", split(\"sensors\", \";\"))\n",
    "df = df.withColumn(\"sensor_item\", explode(\"sensor_list\"))\n",
    "\n",
    "# Step 2: Trim any extra whitespace from each sensor item.\n",
    "df = df.withColumn(\"sensor_item\", trim(\"sensor_item\"))\n",
    "\n",
    "# Step 3: Extract sensor_id, parameter_name, and parameter_units from each sensor_item.\n",
    "#   - sensor_id: the digits before the colon.\n",
    "#   - parameter_name: the token immediately after the colon.\n",
    "#   - parameter_units: the token following the parameter_name.\n",
    "df = df.withColumn(\"sensor_id\", regexp_extract(\"sensor_item\", r\"^\\s*(\\d+):\", 1))\n",
    "df = df.withColumn(\"parameter_name\", regexp_extract(\"sensor_item\", r\":\\s*([^ ]+)\", 1))\n",
    "df = df.withColumn(\"parameter_units\", regexp_extract(\"sensor_item\", r\":\\s*[^ ]+\\s+([^ ]+)\", 1))\n",
    "\n",
    "# Final DataFrame: select all original columns plus the new sensor fields.\n",
    "result_df = df.select(\"location_id\", \"sensors\", \"latitude\", \"longitude\", \"datetime_last\",\n",
    "                        \"sensor_id\", \"parameter_name\", \"parameter_units\").drop(\"sensors\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75eaa1fe-ce55-4875-a26b-a08937e5ed78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "At this point, we need to filter any sensors with Null datetime_Last parameters as well as any that don't have a datetime_last in 2025, as they are no longer active sensors and shouldn't be used for the nearest neighbor calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fa6e9e-07ee-49b2-89d3-787e7e312af8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, lit\n",
    "\n",
    "# Count null columns for the datetimeLast - indicates that the sensor activity is not collecting data\n",
    "result_df.filter(col(\"datetime_last\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ee1fd6e-cf92-4af4-9c2d-24b1f3a7547c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are null values, which indicates that there is no recorded data from those sensors at all. \n",
    "Next, I need to clean up the datetimeLast column - this is the last date that the sensor was active, but it has recorded both utc and local time in the same string within the column. The FIRMS data only works on utc, so I only want to keep this, and I also only want to filter by date, not time. I can use regex to remove the second half of the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c5d986-42fd-44d6-a6a1-6c5fb22aa1dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter the Nulls and Dates that are prior to 2025-01-01, as they are not active for the data range needed\n",
    "filtered_sensors = result_df.filter(\n",
    "    (col(\"datetime_last\").isNotNull()) &  # Exclude rows where datetimeLast_utc is null\n",
    "    (col(\"datetime_last\") >= to_date(lit(\"2025-01-01\")))  # Keep rows with datetimeLast_utc >= 2025-01-01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc451e5f-80e8-4c64-b19e-992c40d55c33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(filtered_sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a36c4bc3-6e15-4ac5-a575-185b9acdf99f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Verify still no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163033f4-0304-4b7c-9d84-78050deca87b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "duplicate_count = filtered_sensors.groupBy(filtered_sensors.columns).count().filter(\"count > 1\").count()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "665e528f-f3b1-4bde-b560-7db5b5f886c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# This will create a new DataFrame where each column shows the number of nulls in that column.\n",
    "null_counts = filtered_sensors.select(\n",
    "    *[F.sum(F.when(F.col(col).isNull(), 1).otherwise(0)).alias(col)\n",
    "      for col in filtered_sensors.columns]\n",
    ")\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0abbbe5-c3c7-4c0c-93ee-3ca333aca310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Option 2: Save the Delta table to an explicit DBFS path and register it.\n",
    "filtered_sensors.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/delta/filtered_sensors\") \\\n",
    "    .saveAsTable(\"filtered_sensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae23afad-0496-48d1-aff5-7442110c2144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM filtered_sensors"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6368848666129508,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Static Layer 2 - Sensor Location Processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
