{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71033c79-78cb-4bdf-977c-f6e45f752906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Perform the Nearest Neighbor Calculations \n",
    "This will open the Filtered Geo Census data and the Sensor Location data on the centroid coordinates and location coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b209a93-ae17-4c23-99ad-2b891978bfe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note: since the computation for the nearest neighbor was intensive, I changed the compute properties of the cluster to r5d.xlarge and included the following parameters in the advanced section: \n",
    "\n",
    "- spark.memory.offHeap.enabled true\n",
    "- spark.driver.memory 16g\n",
    "- spark.executor.instances 4\n",
    "- spark.memory.offHeap.size 2g\n",
    "- spark.driver.cores 4\n",
    "- spark.executor.memory 16g\n",
    "- spark.executor.cores 4\n",
    "\n",
    "This was changed back after this process, since it was a single process and won't need to be performed again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f7473b2-8cd2-478f-b7c2-8d8eccb2aab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filtered_geo_census_path = \"/Volumes/tabular/dataexpert/freshoats_capstone/filtered_geo_census.parquet\"\n",
    "\n",
    "filtered_geo_census_df = spark.read.parquet(filtered_geo_census_path)\n",
    "filtered_sensors = spark.table('filtered_sensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe26328-9c74-4c69-845f-1ca66b6de42b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(filtered_geo_census_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ff1707-59b5-47cd-9a8a-25d5c14718a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporary view of the DataFrames\n",
    "filtered_geo_census_df.createOrReplaceTempView(\"census_tracts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d41172c-2461-4708-851a-2679d41f4344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verify the views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f40172c-462b-4a40-9f8f-0f71c5c6939f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify the census_tracts view\n",
    "spark.sql(\"SELECT * FROM census_tracts LIMIT 5\").show()\n",
    "\n",
    "# Verify the sensor_locations view\n",
    "spark.sql(\"SELECT * \\\n",
    "          FROM filtered_sensors \\\n",
    "          WHERE datetime_last < '2025-01-01' OR datetime_last IS NULL \\\n",
    "          LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b8c836c-6d9f-4468-90c6-900dbcd4b2ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    %sql\n",
    "    WITH sensors AS (\n",
    "      SELECT *\n",
    "          FROM filtered_sensors\n",
    "          WHERE datetime_last >= '2025-01-01' AND datetime_last IS NOT NULL\n",
    "    )\n",
    "      SELECT\n",
    "          c.GEOID AS tract_id\n",
    "          , s.location_id\n",
    "          , 2 * 6371 * ASIN(SQRT(\n",
    "              POWER(SIN(RADIANS(s.latitude - c.INTPTLAT) / 2), 2) +\n",
    "              COS(RADIANS(c.INTPTLAT)) * COS(RADIANS(s.latitude)) *\n",
    "              POWER(SIN(RADIANS(s.longitude - c.INTPTLON) / 2), 2)\n",
    "          )) AS distance\n",
    "      FROM\n",
    "          census_tracts c\n",
    "      CROSS JOIN\n",
    "          sensors s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5595f8e2-857e-46e5-89f4-5207ab48a561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL query to calculate distances and find the sensor distances from each census tract to each sensor using the Haversine formula\n",
    "nearest_neighbors_query = \"\"\"\n",
    "    WITH sensors AS (\n",
    "      SELECT *\n",
    "          FROM filtered_sensors\n",
    "          WHERE datetime_last >= '2025-01-01' AND datetime_last IS NOT NULL\n",
    "    )\n",
    "      SELECT\n",
    "          c.GEOID AS tract_id\n",
    "          , s.location_id\n",
    "          , 2 * 6371 * ASIN(SQRT(\n",
    "              POWER(SIN(RADIANS(s.latitude - c.INTPTLAT) / 2), 2) +\n",
    "              COS(RADIANS(c.INTPTLAT)) * COS(RADIANS(s.latitude)) *\n",
    "              POWER(SIN(RADIANS(s.longitude - c.INTPTLON) / 2), 2)\n",
    "          )) AS distance\n",
    "      FROM\n",
    "          census_tracts c\n",
    "      CROSS JOIN\n",
    "          sensors s\n",
    "    \"\"\"\n",
    "\n",
    "# Execute the query\n",
    "distances_df = spark.sql(nearest_neighbors_query)\n",
    "\n",
    "# Show the calculated distances\n",
    "distances_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aac301f-0115-47c8-8ba6-ff8c3c9c4c52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The cross-join got all of the distances. They are currenlty in no particular order. The next step will be running a Window function to rank partition by the tract and then rank by distance, which by default orders in ascending order. This means that the first row will be the shortest distance, and that will be the nearest neighbor sensor to the tract. \n",
    "\n",
    "Since this step requires ordering and shuffling, it is going to take substantially longer and more computational power to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6bd5f8d-0120-4273-af3a-8865d2eeb4e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Define a window partitioned by tract_id and ordered by distance\n",
    "window = Window.partitionBy(\"tract_id\").orderBy(\"distance\")\n",
    "\n",
    "# Add a rank column and filter for the nearest sensor (rank = 1)\n",
    "nearest_neighbors_df = distances_df.withColumn(\"rank\", row_number().over(window)).filter(\"rank = 1\")\n",
    "\n",
    "# Show the nearest neighbors\n",
    "nearest_neighbors_df.select(\"tract_id\", \"location_id\", \"distance\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a99581b9-1909-44b4-bf84-853d88d45834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a66033-d562-4402-bee7-c1f038e59456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nearest_neighbors_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/delta/nearest_neighbors\") \\\n",
    "    .saveAsTable(\"nearest_neighbors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2c857a-b780-4889-ae0c-c5339fa6707f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nearest_neighbors_df = spark.read.table(\"nearest_neighbors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf3aef5c-0020-4ffc-95d4-4f557b1074d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(nearest_neighbors_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b88155c-05f7-42c7-9d93-1dc3bc14a83b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are 1455 tracts without median income data - this is fine, as I will be grouped into all different income brackets if null. The next step is to set up the marketing brackets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec869a23-ce72-48fb-9b9f-2968451face3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the result as a managed Delta table\n",
    "result_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sensors_with_tract_income_brackets\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6368848666128489,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Static Layer 4 - Nearest Neighbor Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
