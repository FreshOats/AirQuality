{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea144e8-577a-44d3-b751-6cc937db1fc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install awscli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d257104-7ac0-48ec-9bf6-c3db36dbdae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh aws configure set region us-east-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc574ffc-f431-4c84-9a8b-2de02d52e5a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc9764bd-5d9d-4939-b033-abda444e94aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The below backfill only collected files from the LA region. I need to collect from the elsewhere in addition, but I don't need to do a backfill on this right now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e2c98c-29c9-40b1-8f40-059635bd3623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import subprocess\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Initialize the Spark session (Databricks automatically creates one)\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# # Read the Delta table that contains the location_id column.\n",
    "# # Adjust the path as needed for your environment.\n",
    "# delta_df = spark.read.format(\"delta\").load(\"/Volumes/tabular/dataexpert/freshoats_capstone/la_sensors_delta_table\")\n",
    "\n",
    "# # Collect the location_ids into a Python list (assuming the column name is \"location_id\")\n",
    "# location_ids = [row.location_id for row in delta_df.select(\"location_id\").collect()]\n",
    "\n",
    "# # Settings for the AWS S3 backfill\n",
    "# year = '2025'\n",
    "# month = '02'\n",
    "# base_s3_path = \"s3://openaq-data-archive/records/csv.gz/\"\n",
    "\n",
    "# # New folder in your volume where files will be saved\n",
    "# local_base_dir = \"/Workspace/Users/justin.papreck@gmail.com/AirQuality/AirQualityBackfill\"\n",
    "\n",
    "# # Create the new folder if it doesn't already exist\n",
    "# if not os.path.exists(local_base_dir):\n",
    "#     os.makedirs(local_base_dir)\n",
    "\n",
    "# # Pause duration (in seconds) before retrying if an error is encountered\n",
    "# retry_delay_sec = 60\n",
    "\n",
    "# for loc_id in location_ids:\n",
    "#     # Construct the S3 folder path for the specific location_id, year, and month\n",
    "#     s3_path = f\"{base_s3_path}locationid={loc_id}/year={year}/month={month}/\"\n",
    "    \n",
    "#     # Create a subdirectory for this location within the new folder\n",
    "#     local_dest = os.path.join(local_base_dir, f\"location-{loc_id}\")\n",
    "#     if not os.path.exists(local_dest):\n",
    "#         os.makedirs(local_dest)\n",
    "    \n",
    "#     # Build the AWS CLI command\n",
    "#     command = f\"aws s3 cp --no-sign-request --recursive {s3_path} {local_dest}\"\n",
    "#     print(f\"Starting download for location {loc_id}\")\n",
    "#     print(f\"Executing: {command}\")\n",
    "    \n",
    "#     # Retry loop: if the command exits with a nonzero code (error), pause and retry.\n",
    "#     while True:\n",
    "#         result = subprocess.run(command, shell=True)\n",
    "#         if result.returncode == 0:\n",
    "#             print(f\"Download for location {loc_id} completed successfully.\\n\")\n",
    "#             break\n",
    "#         else:\n",
    "#             print(f\"Error encountered for location {loc_id}. Pausing for {retry_delay_sec} seconds before retrying...\\n\")\n",
    "#             time.sleep(retry_delay_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7e6bcb6-1c47-4901-ae6c-731d56a89de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, open and append these files to a table for sensor measurements, that will function as the primary table to write to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b51dfa84-26cb-4827-9179-248c9cbd4931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Define the base directory where all location folders are stored\n",
    "base_dir = \"/Workspace/Users/justin.papreck@gmail.com/AirQuality/AirQualityBackfill\"\n",
    "\n",
    "# Define the target directory where decompressed CSV files will be saved\n",
    "# This directory will be: AirQualityBackfill/Decompressed/\n",
    "target_dir = os.path.join(base_dir, \"Decompressed\")\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over each folder in the base directory\n",
    "for folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    \n",
    "    # Process only folders that represent locations (and skip the target directory)\n",
    "    if os.path.isdir(folder_path) and folder.startswith(\"location-\"):\n",
    "        # Iterate over each file in the location folder\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".gz\"):\n",
    "                gz_file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Remove the '.gz' extension to get the CSV file name\n",
    "                csv_file_name = file_name[:-3]\n",
    "                # Prefix with the location folder name to ensure unique names\n",
    "                output_csv_filename = f\"{folder}-{csv_file_name}\"\n",
    "                \n",
    "                # Define the full output path in the target directory\n",
    "                csv_file_path = os.path.join(target_dir, output_csv_filename)\n",
    "                \n",
    "                # Decompress only if the CSV file does not already exist\n",
    "                if not os.path.exists(csv_file_path):\n",
    "                    print(f\"Decompressing {gz_file_path} to {csv_file_path} ...\")\n",
    "                    with gzip.open(gz_file_path, 'rb') as f_in, open(csv_file_path, 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "                    print(f\"Decompressed to {csv_file_path}\")\n",
    "                else:\n",
    "                    print(f\"CSV file already exists: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163fc272-408f-49f4-88af-c733d9a8be3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or retrieve the Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define the local directory where the decompressed CSV files are located.\n",
    "local_decompressed_dir = \"/Workspace/Users/justin.papreck@gmail.com/AirQuality/AirQualityBackfill/Decompressed\"\n",
    "\n",
    "# Define the target directory in DBFS.\n",
    "dbfs_target_dir = \"dbfs:/FileStore/Decompressed\"\n",
    "\n",
    "# Create the DBFS directory if it doesn't already exist.\n",
    "dbutils.fs.mkdirs(dbfs_target_dir)\n",
    "\n",
    "# Iterate over every file in the local decompressed directory.\n",
    "for file_name in os.listdir(local_decompressed_dir):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        # Construct the full source and destination paths.\n",
    "        source_path = f\"file:{local_decompressed_dir}/{file_name}\"\n",
    "        destination_path = f\"{dbfs_target_dir}/{file_name}\"\n",
    "        print(f\"Copying {source_path} to {destination_path}...\")\n",
    "        # Copy the file from the local filesystem to DBFS.\n",
    "        dbutils.fs.cp(source_path, destination_path)\n",
    "        print(f\"Copied {file_name} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c920267-dc9a-4e0c-9712-81562a1151a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE  sensor_measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ede08e-7b95-4282-a11b-7ea5ebe040a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read all CSV files from the DBFS directory (using a wildcard pattern)\n",
    "df_all = spark.read.option(\"header\", \"true\") \\\n",
    "                   .option(\"inferSchema\", \"true\") \\\n",
    "                   .csv(\"dbfs:/FileStore/Decompressed/*.csv\")\n",
    "\n",
    "# Optionally, check the count or preview the data\n",
    "print(\"Total rows read:\", df_all.count())\n",
    "df_all.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a12d09d-abc7-49ed-af32-a79021b8d60c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cb54c84-058e-40c2-91c1-1bf1de7a1aa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_all.write.format(\"delta\").mode(\"append\").saveAsTable(\"sensor_measurements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0099db46-2179-4901-b518-e8f7dc07ccdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM sensor_measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "827cfd67-f524-4326-8de9-791c2f63ab00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Prepare the Backfill data for the permanent measurement table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0cac6a8-2712-4172-8714-b6b402262306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "backfill_df = spark.read.table(\"sensor_measurements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efbdea77-c96c-41b6-b71c-3af1c17950ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(backfill_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8d5caf-17cf-4a56-b5d0-521f41d57b17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "backfill_filtered = backfill_df.withColumnRenamed(\"sensors_id\", \"sensor_id\").select(\"location_id\", \"sensor_id\", \"parameter\", \"units\", \"value\", \"datetime\", \"lat\", \"lon\")\n",
    "display(backfill_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "893687f6-35ee-4a99-8418-b58f0b00a6bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Validate Idempotence before creating Permanent Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa64cff9-1642-4f21-b55e-ce6a14bb2c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate that your enriched data has no duplicates\n",
    "duplicate_check_df = backfill_filtered.groupBy(\"location_id\", \"sensors_id\", \"datetime\").count().filter(\"count > 1\")\n",
    "\n",
    "if duplicate_check_df.count() > 0:\n",
    "    print(\"Duplicates detected in the enriched data:\")\n",
    "    duplicate_check_df.show()\n",
    "else:\n",
    "    print(\"No duplicates found. Data is idempotent based on the common key and timestamp.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af4c4dc8-3838-4d3e-b975-877fc53d35da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "backfill_filtered.write.format(\"delta\").mode(\"append\").saveAsTable(\"permanent_sensor_measurements\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6288841831501046,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Static Layer 6 - Backfill",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
