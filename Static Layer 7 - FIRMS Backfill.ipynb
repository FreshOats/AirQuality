{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9922121b-b389-4ebe-a8b9-b9fb46ec6fda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingestion of NASA FIRMS Fire Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d64b97-3f5d-402c-8484-31c483dc9479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Data Retrieval and Storage in the Bronze Layer**\n",
    "The Bronze Layer is where raw data is ingested and stored. For FIRMS data, this involves:\n",
    "- Pulling data from the FIRMS API for the last 2 weeks.\n",
    "- Appending new data to the Delta Table while ensuring no duplicates are created.\n",
    "\n",
    "**Key Steps for Bronze Layer Implementation**\n",
    "1. Define the Schema for the Delta Table:\n",
    "- The schema should match the FIRMS data fields (e.g., latitude, longitude, acq_date, acq_time, etc.).\n",
    "- Add a unique identifier column to ensure idempotency (e.g., a combination of latitude, longitude, acq_date, and acq_time).\n",
    "2. Retrieve Data from FIRMS API:\n",
    "- Pull data for the last 2 weeks during the initial load.\n",
    "- For subsequent loads, pull only the latest data (e.g., for the last 1 day or 12 hours).\n",
    "3. Ensure Idempotency:\n",
    "- Use a primary key or unique identifier to prevent duplicate rows when appending new data.\n",
    "- A good candidate for the unique identifier is a combination of latitude, longitude, acq_date, and acq_time.\n",
    "4. Save Data to Delta Table:\n",
    "Append new data to the Delta Table while ensuring no duplicates are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1011b699-2ea1-4889-a418-2734d18957b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write the Backfill data to delta table, all_regions_df. Delta Table will ensure Idempotency with future merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e389989d-4361-4601-acb1-c5048fc781ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Backfill data from 2025-02-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50973d65-f2ed-459f-9534-fb58199759ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "102a0bf2-277a-4d3c-8d7f-82a0962ffd41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"FIRMS Delta Table\").getOrCreate()\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "api_key = os.getenv('FIRMS_API_KEY')\n",
    "\n",
    "# API details\n",
    "api_key = api_key\n",
    "dataset = \"VIIRS_NOAA20_NRT\"  # VIIRS NOAA-20 Near Real-Time data\n",
    "country_code = \"USA\"          # Country code for the United States\n",
    "days = 10                     # Number of days of data\n",
    "start_date = '2025-02-01'     # Start date for the data\n",
    "\n",
    "# Construct the API URL for the backfill\n",
    "api_url = f'https://firms.modaps.eosdis.nasa.gov/api/country/csv/{api_key}/{dataset}/{country_code}/{days}/{start_date}'\n",
    "\n",
    "# Fetch the data and load it into a Pandas DataFrame\n",
    "df_us = pd.read_csv(api_url)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df_us)\n",
    "\n",
    "# Set the table name to be used in the metastore\n",
    "table_name = \"firms_data\"\n",
    "\n",
    "# Create the managed Delta table by overwriting any existing instance.\n",
    "spark_df.write.format(\"delta\") \\\n",
    "    .partitionBy(\"acq_date\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "# Optimize the table with ZORDER (if supported in your environment)\n",
    "spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (longitude)\")\n",
    "\n",
    "print(\"Static backfill completed: Table created and optimized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b35fc4de-7a3c-403d-8073-d46abfe878b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT MAX(acq_date)\n",
    "FROM firms_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1768b019-6dd8-4ccc-85c9-b366eb5d8aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The next code adds the merge function; this was run twice on the 10th and 20th, producing two duplicate dates for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c209077-a8e7-4a12-a555-997d398e8019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"FIRMS Delta Table\").getOrCreate()\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "api_key = os.getenv('FIRMS_API_KEY')\n",
    "\n",
    "# API details\n",
    "api_key = api_key\n",
    "dataset = \"VIIRS_NOAA20_NRT\"  # VIIRS NOAA-20 Near Real-Time data\n",
    "country_code = \"USA\"          # Country code for the United States\n",
    "days = 10                     # Number of days of data\n",
    "start_date = '2025-02-20'     # Start date for the data\n",
    "\n",
    "# Construct the API URL for the backfill\n",
    "api_url = f'https://firms.modaps.eosdis.nasa.gov/api/country/csv/{api_key}/{dataset}/{country_code}/{days}/{start_date}'\n",
    "\n",
    "# Fetch the data and load it into a Pandas DataFrame\n",
    "df_us = pd.read_csv(api_url)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df_us)\n",
    "\n",
    "# Set the table name to be used in the metastore\n",
    "table_name = \"firms_data\"\n",
    "\n",
    "# Since the table already exists from the initial static load, perform a merge.\n",
    "# The matching condition ensures that if a row with the same:\n",
    "#   latitude, longitude, acq_date, and acq_time already exists, it will be updated.\n",
    "# This takes care of the overlapping day data and prevents duplicates.\n",
    "if spark.catalog.tableExists(table_name):\n",
    "    delta_table = DeltaTable.forName(spark, table_name)\n",
    "    \n",
    "    delta_table.alias(\"target\").merge(\n",
    "        spark_df.alias(\"source\"),\n",
    "        \"\"\"\n",
    "        target.latitude = source.latitude AND\n",
    "        target.longitude = source.longitude AND\n",
    "        target.acq_date = source.acq_date AND\n",
    "        target.acq_time = source.acq_time\n",
    "        \"\"\"\n",
    "    ).whenMatchedUpdateAll() \\\n",
    "     .whenNotMatchedInsertAll() \\\n",
    "     .execute()\n",
    "\n",
    "    # Optimize the table if desired\n",
    "    spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (longitude)\")\n",
    "else:\n",
    "    # This branch would only run if the table doesn't exist.\n",
    "    # It creates a managed Delta table (with partitioning by acq_date) and optimizes it.\n",
    "    spark_df.write.format(\"delta\") \\\n",
    "        .partitionBy(\"acq_date\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(table_name)\n",
    "    spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (longitude)\")\n",
    "\n",
    "print(\"Data merged and Delta table optimized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b83f5bbd-25e9-4b25-bfa0-0e6ab3e32ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT MAX(acq_date)\n",
    "FROM firms_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dc26a47-0907-41f6-9446-19e949fc357c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The next code will be the code input into the Bronze layer that collects the most recent data; it will include overlapping days in case there is a late update up to 3 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7432c0c0-461f-4cad-91a7-e8de72eb3282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "api_key = os.getenv('FIRMS_API_KEY')\n",
    "\n",
    "# API details\n",
    "api_key = api_key\n",
    "dataset = \"VIIRS_NOAA20_NRT\"  # VIIRS NOAA-20 Near Real-Time data\n",
    "country_code = \"USA\"          # Country code for the United States\n",
    "days = 4                     # Number of days of data\n",
    "\n",
    "# Construct the API URL\n",
    "api_url = f'https://firms.modaps.eosdis.nasa.gov/api/country/csv/{api_key}/{dataset}/{country_code}/{days}'\n",
    "\n",
    "# Fetch the new data from the FIRMS API\n",
    "new_data_df = pd.read_csv(api_url)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spark DataFrame\n",
    "new_spark_df = spark.createDataFrame(new_data_df)\n",
    "\n",
    "# Set the table name to be used in the metastore\n",
    "table_name = \"firms_data\"\n",
    "\n",
    "# Check if the table already exists using the Spark catalog\n",
    "if spark.catalog.tableExists(table_name):\n",
    "    # Load the existing Delta table using its table name\n",
    "    delta_table = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "    # Merge new data with the existing table\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        new_spark_df.alias(\"source\"),\n",
    "        \"\"\"\n",
    "        target.latitude = source.latitude AND\n",
    "        target.longitude = source.longitude AND\n",
    "        target.acq_date = source.acq_date AND\n",
    "        target.acq_time = source.acq_time\n",
    "        \"\"\"\n",
    "    ).whenMatchedUpdateAll() \\\n",
    "     .whenNotMatchedInsertAll() \\\n",
    "     .execute()\n",
    "\n",
    "    # Optimize the table (if your environment supports it)\n",
    "    spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (longitude)\")\n",
    "else:\n",
    "    # If the table doesn't exist, create it as a managed Delta table with partitioning\n",
    "    new_spark_df.write.format(\"delta\") \\\n",
    "        .partitionBy(\"acq_date\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(table_name)\n",
    "\n",
    "    # Optimize after initial write\n",
    "    spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (longitude)\")\n",
    "\n",
    "print(\"Data merged and Delta table optimized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e94d5a1-1763-4552-b321-4e485cb01a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT max(acq_date)\n",
    "FROM firms_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e7e5372-609f-403b-bf2f-0ec4c5517c99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Check for Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0634a0b1-5fc1-48b3-8ffc-c7ab1056577b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  latitude, \n",
    "  longitude, \n",
    "  acq_date, \n",
    "  acq_time, \n",
    "  COUNT(*) AS duplicate_count\n",
    "FROM firms_data\n",
    "GROUP BY \n",
    "  latitude, \n",
    "  longitude, \n",
    "  acq_date, \n",
    "  acq_time\n",
    "HAVING COUNT(*) > 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce221ac7-ee16-4629-a033-4d7b6532478b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Verify all datas are filled between min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1761fd1-c074-4f90-8cfc-ba396972d4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH min_max AS (\n",
    "  SELECT\n",
    "    MIN(CAST(acq_date AS DATE)) AS min_date,\n",
    "    MAX(CAST(acq_date AS DATE)) AS max_date\n",
    "  FROM firms_data\n",
    "),\n",
    "all_dates AS (\n",
    "  SELECT EXPLODE(sequence(min_date, max_date, interval 1 day)) AS dt\n",
    "  FROM min_max\n",
    "),\n",
    "existing_dates AS (\n",
    "  SELECT DISTINCT CAST(acq_date AS DATE) AS dt\n",
    "  FROM firms_data\n",
    ")\n",
    "SELECT a.dt AS missing_date\n",
    "FROM all_dates a\n",
    "LEFT JOIN existing_dates e\n",
    "  ON a.dt = e.dt\n",
    "WHERE e.dt IS NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7583122e-13cf-46b0-b76c-293e2935bc62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0d43435-d0f7-436f-abc0-afc1b039042d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We have an idempotent data ingestion for the FIRMS data with all dates backfilled to 2025-02-01"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7451663135488457,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Static Layer 7 - FIRMS Backfill",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
